# -*- coding: utf-8 -*-
"""CNN T-S Binary Classifier

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u6a2yRNFcSCeBOY4dSgfeRcbhBDP-GHv
"""

from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
np.set_printoptions(suppress=True)
import random
from scipy import stats



def synthesize(mag = None, magerr = None, time = None, ceph_LC = 1, eb_LC = 1, N = None, printt = 0):

    def seed_gen(N):
        N = int(N)
        eemag = 0.02 + 0.1 * np.random.rand(N)
        mmag = np.random.normal(loc = random.uniform(11, 16), scale = eemag)
        ttime = np.array(sorted(list(np.random.uniform(0.5, random.uniform(800, 2000), N))))
        return mmag, eemag, ttime
    

    if mag == None and time == None and magerr == None:
        if N == None:
            N = random.uniform(50, 200)
        mag, magerr, time = seed_gen(N)


    random_synth = 0
    if ceph_LC == 1 and eb_LC == 1:
        random_synth = 1
    # set up an example seed light curve
    time_range = max(np.array(time)) - min(np.array(time))

    m0 = np.median(mag)
    n_pts = int(len(mag))

    #either use real of generated mag and mag error as seed
    #use generated if seed is causing problems (likely source LC is variable itself)
    seed_t = time
    seed_em = magerr    # 0.02 + 0.1 * np.random.rand(n_pts)
    seed_m = mag        #np.random.normal(loc = m0, scale = seed_em)



    if random_synth == 1:                
    #IF BOTH ARE SELECTED, RANDOMLY CHOOSE ONE
        if random.random()< 0.5:
        #if np.random.randint(2, size=1) == 1:
            ceph_LC = 1
            eb_LC = 0
        else:
            ceph_LC = 0
            eb_LC = 1


    #VVV NB P and A are pulled out of thin air, no real justification apart from basic astro
    #P = random.uniform(0.1, 1000)#time_range*0.5)
    if random.random()< 0.1:   #fuck it, 10% its a big amplitde
        P = random.uniform(1, time_range)#time_range*0.5)
    if random.random()< 0.1:   #fuck it, 10% its a big amplitde
        P = random.uniform(0.1, 10)#time_range*0.5)
    if random.random()< 0.1:   #fuck it, 10% its a big amplitde
        P = random.uniform(100, 1000)#time_range*0.5)
    else:   #fuck it, 10% its a big amplitde
        P = random.uniform(0.1, 100)#time_range*0.5)


    P = random.uniform(0.1, 10)#time_range*0.5)

    Amplitude = random.uniform(1, 4)
    
    #======================
    #SINUSOIDAL LC
    #======================
    if ceph_LC == 1:
        lc_model = lambda t, m: m0 - ((0.5*np.sin((2*np.pi*t)/P) - 0.15*np.sin((2*2*np.pi*t)/P) - 0.05*np.sin((3*2*np.pi*t)/P)))*Amplitude
        cat_type = 'Ceph'


    #================
    #EB LC
    #================
    if eb_LC == 1:
        A1 = random.uniform(0.1, Amplitude)#time_range*0.5)
        A2 = Amplitude - A1#random.uniform(0.1, 100)#time_range*0.5)
        lc_model = lambda t, m: m0 - (A1*np.sin((2*np.pi*t)/P)**2 - A2*np.sin((np.pi*t)/P)**2)
        cat_type = 'EB'





    sim_mags = lc_model(seed_t, seed_m)
    if printt == 1:
      print('  \=================================================/\n#God-Given Period = ',P,'\n#God-Given Amplitude = ',str(round(Amplitude,2)),'\n#N = ',str(len(sim_mags)),'\n#Class = ',cat_type,'\n')

    return sim_mags, seed_em, seed_t, cat_type



samples = 5000
datapoints = 5000
epochs = 500
batch_size = 32

x_test = []
y_test = []
for i in range(samples) :
    mag, magerr, time, cat_type = synthesize(N = datapoints)
    x_test.append(np.vstack((stats.zscore(mag), stats.zscore(magerr), time)))
    y_test.append(cat_type)
y_test = [0 if element == 'EB' else 1 for element in y_test]
y_test = np.array(y_test)
x_test = np.array(x_test)

x_train = []
y_train = []
for i in range(samples) :
    mag, magerr, time, cat_type = synthesize(N = datapoints)
    x_train.append(np.vstack((stats.zscore(mag), stats.zscore(magerr), time)))
    y_train.append(cat_type)

y_train = [0 if element == 'EB' else 1 for element in y_train]
y_train = np.array(y_train)
x_train = np.array(x_train)

classes = np.unique(y_train)
eb_x_train = x_train[np.where(np.array(y_train) == classes[1])[0][0]]
c_x_train = x_train[np.where(np.array(y_train) == classes[0])[0][0]]

plt.plot(eb_x_train[2],eb_x_train[0], 'kx', label=classes[1])

plt.plot(c_x_train[2],c_x_train[0], 'bx', label=classes[0])

plt.legend(loc="best")
plt.show()
plt.close()

num_classes = len(classes)

def make_model(input_shape):
    input_layer = keras.layers.Input(input_shape)

    conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding="same")(input_layer)
    conv1 = keras.layers.BatchNormalization()(conv1)
    conv1 = keras.layers.ReLU()(conv1)

    conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding="same")(conv1)
    conv2 = keras.layers.BatchNormalization()(conv2)
    conv2 = keras.layers.ReLU()(conv2)

    conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding="same")(conv2)
    conv3 = keras.layers.BatchNormalization()(conv3)
    conv3 = keras.layers.ReLU()(conv3)

    gap = keras.layers.GlobalAveragePooling1D()(conv3)

    output_layer = keras.layers.Dense(num_classes, activation="softmax")(gap)

    return keras.models.Model(inputs=input_layer, outputs=output_layer)

model = make_model(input_shape=x_train.shape[1:])
keras.utils.plot_model(model, show_shapes=False)

callbacks = [
    keras.callbacks.ModelCheckpoint(
        "best_model.h5", save_best_only=True, monitor="val_loss"
    ),
    keras.callbacks.ReduceLROnPlateau(
        monitor="val_loss", factor=0.5, patience=20, min_lr=0.0001
    ),
    keras.callbacks.EarlyStopping(monitor="val_loss", patience=50, verbose=1),
]
model.compile(
    optimizer="adam",
    loss="sparse_categorical_crossentropy",
    metrics=["sparse_categorical_accuracy"],
)
history = model.fit(
    x_train,
    y_train,
    batch_size=batch_size,
    epochs=epochs,
    callbacks=callbacks,
    validation_split=0.2,
    verbose=1,
)

model = keras.models.load_model("best_model.h5")

test_loss, test_acc = model.evaluate(x_test, y_test)

print("Test accuracy", test_acc)
print("Test loss", test_loss)

metric = "sparse_categorical_accuracy"
plt.figure()
plt.plot(history.history[metric])
plt.plot(history.history["val_" + metric])
plt.title("model " + metric)
plt.ylabel(metric, fontsize="large")
plt.xlabel("epoch", fontsize="large")
plt.legend(["train", "val"], loc="best")
plt.show()
plt.close()

x_test = []
y_test = []
for i in range(1) :
    mag, magerr, time, cat_type = synthesize(N = datapoints, printt = 1)
    x_test.append(np.vstack((stats.zscore(mag), stats.zscore(magerr), time)))
    y_test.append(cat_type)
y_test = [0 if element == 'EB' else 1 for element in y_test]
y_check = np.array(y_test)
x_check = np.array(x_test)

model.predict(x_check)